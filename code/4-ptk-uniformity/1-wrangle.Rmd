---
title: "1. /ptk/ uniformity - wrangle"
author: "Khia A Johnson"
date: "3/31/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r import}
library(tidyverse)
library(tidylog, warn.conflicts = FALSE)
library(DBI)
```

Load the ptk dataframe extracted from spice.db with both forced alignment and autovot tables
```{r}
df <- read_csv('ptk_20210324.csv')
```

Load pronunciation dictionary data (saved outside of repo, available with SpiCE corpus), combine into a single dictionary and clean up.
```{r}
cantodict <- read_tsv('~/Corpora/spice/info/alignment/cantonese_pronunciation.dict', 
                      col_names = c('word', 'pronunciation'))

librespeech <- read_csv('~/Corpora/spice/info/alignment/english_pronunciation.dict', 
                        col_names = c('item'))

librespeech <- librespeech %>%
  mutate(item = str_replace_all(item, '\\s+', ' ')) %>%
  mutate(item = str_replace(item, ' ', '#')) %>%
  separate(item, into = c('word', 'pronunciation'), sep = '#') %>%
  mutate(word = str_to_lower(word))

pd <- rbind(cantodict, librespeech)
rm(cantodict, librespeech)
```

Keep only the longest entry for each unique word and get segment count
```{r}
pd <- pd %>%
  mutate(spaces = str_count(pronunciation, ' ')) %>%
  arrange(desc(spaces)) %>%
  distinct(word, .keep_all = TRUE) %>%
  arrange(pronunciation) %>% 
  mutate(n_phones_canonical = spaces+1) %>% 
  select(word, pronunciation, n_phones_canonical)
```

Fold in pronunciation and number of segments in canonical production
```{r}
df <- df %>%
  left_join(pd, by='word') 
```


Connect to SpiCE SQLite database
```{r}
db <- dbConnect(RSQLite::SQLite(), "../1-general/spice.db")
dbListTables(db)
```

Load the words and tasks tables, and join
```{r}
tasks <- dbReadTable(db, "tasks")
interview_onset_times <- tasks %>%
  filter(task=='interview') %>%
  separate(file, into=c('talker','language')) %>%
  select(talker, language, interview_onset_s=task_onset)
```

Calculate variables of interest, ditch any pre-interview tokens, and word fragments -- things that shouldn't have made it into the list to begin with
```{r}
df <- df %>%
  separate(file, into = c('talker', 'language', 'order', 'date'), sep = '_') %>%
  left_join(interview_onset_times, by=c('talker','language')) %>%
  filter(avot_onset > interview_onset_s) %>%
  filter(!word %in% c('th', 't', 'p', 'k', 'ta', 'ca', 'co', 'cu', 'kee', 'pa', 'ti', 'ts', 'com', 'commu', 'cou', 'coul', 'countr', 'ki', 'kok', 'kur', 'pir', 'tak', 'tal', 'tol', 'tuh', 'typic')) %>%
  mutate(vot_ms = as.integer(round(1000*(avot_offset-avot_onset)))) %>%
  mutate(avg_phone_dur_ms = as.integer(round(1000*(word_offset-avot_onset)/n_phones_canonical))) %>%
  mutate(is_post_pausal = prev_phone %in% c('sp','sil')) %>%
  select(talker, language, segment=avot_phone, vot_ms, is_post_pausal, following_vowel=next_phone, avg_phone_dur_ms, prev_word, word, pronunciation)
```

What proportion of the sample is Cantonese before filtering?
```{r}
df %>%
  group_by(language) %>%
  summarise(n=n()) %>%
  pivot_wider(names_from=language, values_from=n) %>%
  mutate(prop_Canto = Cantonese / (Cantonese+English))
```

Filter out likely errors and keep track of info to report
```{r}
initial_count <- nrow(df)
message('Starting at: ', initial_count)

df <- df %>%
  tidylog::filter(!is.na(prev_word)) %>%
  tidylog::filter(prev_word != '<unk>') %>%
  tidylog::filter(vot_ms > 15) %>%
  tidylog::mutate(upper_threshold = mean(vot_ms) + 2.5*sd(vot_ms) ) %>%
  tidylog::filter(vot_ms <= upper_threshold) %>%
  tidylog::filter(word!='to') # Chodroff & Wilson (2017) do this

message('upper_threshold value is: ', unique(df$upper_threshold))
message('total proportion removed: ', 1-(nrow(df)/initial_count))
```

What proportion of the sample is Cantonese after filtering?
```{r}
df %>%
  group_by(language) %>%
  summarise(n=n()) %>%
  pivot_wider(names_from=language, values_from=n) %>%
  mutate(prop_Canto = Cantonese / (Cantonese+English+2295)) #fold back in 'to' tokens here
```

Check for rows with NAs
```{r}
df %>%
  filter_all(is.na)
```

Count ptk by language
```{r}
df %>%
  group_by(language, segment) %>%
  summarise(n=n()) %>%
  ungroup() %>%
  pivot_wider(names_from = 'segment', values_from='n') %>%
  select(language, p, t, k)
```

Explora why Cantonese /p/ counts are lower
```{r}
df %>%
  group_by(language, segment, word) %>%
  summarise(n=n()) %>%
  filter(segment=='p') %>%
  group_by(language) %>%
  summarise(word_types=n(), min(n), median(n),mean(n),max(n))
```

Why are English /k/'s higher?
```{r}
df %>%
  group_by(language, segment, word) %>%
  summarise(n=n()) %>%
  arrange(desc(n)) 
```

check stress
```{r}
df %>%
  filter(language=='English') %>%
  group_by(segment, word, pronunciation) %>%
  summarise(n=n()) %>%
  arrange(desc(n)) %>%
  filter(str_detect(pronunciation, '^[A-Z]+ [A-Z]+0'))
```

cuz       419
together  99
toronto   39
piano     30
canadian  18
career    17
compared  14
percent   12
towards   12


segment   sum(n)
k	        661			
p	        97			
t	        167	
total       925

Range of stop counts by talker
```{r}
df %>%
  group_by(talker, language) %>%
  summarise(n=n())  %>%
  ungroup() %>%
  group_by(language) %>%
  summarise(min(n), median(n), mean(n), max(n))

```


Select columns to keep and output clean data frame
```{r}
df %>%
  select(talker, language, segment, following_vowel, vot_ms, is_post_pausal, avg_phone_dur_ms, word) %>%
  # write_csv('ptk_clean_20210331.csv')
  write_csv('ptk_clean_20210601.csv')
```




