---
title: "SpiCE Descriptive Statistics"
author: "Khia A. Johnson"
date: "5/21/2021"
output: html_document
---

# Chapter 2: The SpiCE Corpus

This file accompanies chapter 2 of my dissertation. It includes descriptive statistics and data visualization summarizing the participants and contents of the corpus in broad strokes.

---
## Setup
Import packages
```{r message=FALSE}
library(tidyverse)
library(ggthemes)
library(GGally)
# library(tidylog)
library(DBI)
library(viridis)
```

Set visualization defaults
```{r}
theme_set(theme_clean() + theme(
      legend.title = element_text(face = 'bold', size=10),
      legend.text = element_text(size=10),
      legend.position = 'right',
      axis.title.x = element_text(size = 10, face='bold'),
      axis.title.y = element_text(size = 10, face='bold')
    ))

# ggplot <- function(...) ggplot2::ggplot(...) + 
#     scale_color_colorblind() +
#     scale_fill_colorblind()
```

Connect to SpiCE SQLite database
```{r}
db <- dbConnect(RSQLite::SQLite(), "../1-general/spice.db")
dbListTables(db)
```

Load the words and tasks tables, and join
```{r}
words <- dbReadTable(db, "words")
tasks <- dbReadTable(db, "tasks")

tasks
```

Query the full word table merged with task by time stamps
```{r}
results <- dbSendQuery(
    db,
    "SELECT word,
	       word_onset,
	       word_offset,
	       task,
	       task_onset,
	       task_offset,
	       words.file
	FROM words
	LEFT JOIN tasks ON (tasks.file = words.file
	                    AND word_onset BETWEEN task_onset AND task_offset)"
    )
words <- fetch(results)

words <- words %>%
    separate(file, into = c('talker','language','order','date'), remove = FALSE)
```

## Corpus Summary
Plot log word frequency for both languages, excluding instances of <unk>
```{r fig.width=6, fig.height=3}
words %>%
    filter(word != '<unk>') %>%
    group_by(language, word) %>%
    summarise(frequency = n()) %>%
    ungroup() %>%
    mutate(log_frequency = log(frequency)) %>%
    ggplot(aes(x=log_frequency)) + 
        geom_histogram(binwidth = 0.5) + 
        facet_wrap(~language)
```

Hours of actual participant speech production -- excludes all pauses as this number is based purely on where the forced aligner identified words
```{r}
words %>%
    mutate(word_dur = word_offset - word_onset) %>%
    group_by(language) %>%
    summarise(sum(word_dur)/60/60)
```

Proportion of unknown words by talkers; an estimate of code-switching, as unknown words are almost always words in a different language.
The first plots shows the proportion based on a count, and the second based on duration.
```{r}
unk_count <- words %>%
    mutate(is_unknown = word == '<unk>') %>%
    group_by(talker, language, is_unknown) %>%
    summarise(count_words = n()) %>%
    pivot_wider(names_from = is_unknown, values_from=count_words) %>%
    mutate(proportion_unknown = `TRUE`/(`TRUE`+`FALSE`)) %>%
    select(talker, language, proportion_unknown) %>%
    pivot_wider(names_from=language, values_from=proportion_unknown) 

ggparcoord(unk_count, columns = 2:3,
    scale="globalminmax",
    showPoints = TRUE,
    alphaLines = 0.5, 
    )

unk_duration <- words %>%
    mutate(is_unknown = word == '<unk>', word_dur = word_offset-word_onset) %>%
    group_by(talker, language, is_unknown) %>%
    summarise(duration = sum(word_dur)) %>%
    pivot_wider(names_from = is_unknown, values_from=duration) %>%
    mutate(proportion_unknown = `TRUE`/(`TRUE`+`FALSE`)) %>%
    select(talker, language, proportion_unknown) %>%
    pivot_wider(names_from=language, values_from=proportion_unknown) 

ggparcoord(unk_duration, columns = 2:3,
    scale="globalminmax",
    showPoints = TRUE,
    alphaLines = 0.5, 
    )
```

Clear and close the database
```{r}
dbClearResult(results)
dbDisconnect(db)
rm(db, results)
```

## Participant summary

Load the data
```{r message=FALSE, warning=FALSE}
lbq_summary <- read_csv('~/Corpora/spice/info/participants/spice-lbq-summary.csv')
lbq_raw <- read_csv('~/Corpora/spice/info/participants/spice-lbq-detailed.csv')
```

```{r}
demographics <- lbq_raw %>% 
    select(!matches("language"), -X253) 
```

```{r}
demographics %>%
    select(id, starts_with('Places')) %>%
    mutate(across(where(is.character), 
                  ~ str_replace_all(., 'BC |AB |ON |Quebec|CA |London |Macau|Kansai |Kanto ', ''))) %>%
    pivot_longer(-id, names_to = 'Ages') %>%
    mutate(Ages = as_factor(str_replace(Ages, 'Places lived ages', 'Ages'))) %>%
    separate(value, into = c('a','b','c'), sep = ' - ') %>%
    pivot_longer(cols = a:c, values_to = 'Place') %>%
    select(id, Place, Ages) %>%
    arrange(Place) %>%
    drop_na() %>%
    ggplot(aes(y=fct_rev(Place), fill=Ages)) +
        geom_bar() +
        facet_wrap(~Ages) +
        ylab('Country Lived In') +
        xlab('Count') +
        theme(legend.position = 'none')
    
```

```{r}
demographics %>%
    select(id, contains('Caretaker')) %>%
    pivot_longer(-id, values_to = 'Place') %>%
    mutate(name = str_replace(name, 'Caretakers - Region ','')) %>%
    separate(name, into = c('Question', 'Caretaker'), sep = ' - ') %>%
    pivot_wider(names_from = 'Question', values_from = 'Place') %>%
    drop_na() %>%
    mutate(born = str_replace(born,'Manila|Ilocos ', '')) %>%
    mutate(born = str_replace(born,'[A-Za-z]+ China', 'China')) %>%
    group_by(born) %>%
    summarise(n=n()) %>%
    arrange(born) %>%
    
    ggplot(aes(x=n, y=reorder(born, n))) +
        geom_col() +
        xlab('Count') +
        ylab('Caretaker(s) born in')

```


```{r}
lang_nums <- lbq_raw %>% 
    select(id, matches("^Language [0-9]$")) %>%
    pivot_longer(`Language 1`:`Language 9`, names_to = "lang_num", values_to = "language") %>%
    drop_na()

tidy_languages <- lbq_raw %>% 
    select(id, matches("- Language [0-9]$")) %>%
    mutate(across(where(is.character), ~recode(.,`Excellent` = '4', 
                                               `Good` = '3',
                                               `Fair` = '2',
                                               `Elementary` = '1',
                                               `No proficiency` = NULL,
                                               `Daily` = '365',
                                               `Weekly` = '52',
                                               `Monthly` = '12',
                                               `Yearly` = '1',
                                               `Less than once a year` = '0.5',
                                               `Never` = NULL,
                                               `n/a` = NULL
                                               ))) %>%
    mutate(across(-id, as.numeric)) %>%
    pivot_longer(-id) %>%
    separate(name, into = c('question','lang_num'), sep = -10 , extra = 'merge') %>%
    drop_na() %>%
    left_join(lang_nums, by = c('id','lang_num')) %>%
    select(id, question, language, value) %>%
    mutate(question = str_replace(question, ' - $', ''),
           question = str_replace(question, '/', 'or'))

rm(lang_nums)
```

Visualize the age that each participant started learning each of their languages
```{r fig.height=9, fig.width=7}
tidy_languages %>%
    filter(question=='Age Started') %>%
    select(Participant=id, Language=language,Age=value) %>%
    mutate(Age = if_else(Age>=20, 20, Age)) %>%
    arrange(Language, Age) %>%
    ggplot(aes(y=Age, x='', color = Age, label=Language)) +
        geom_point(size=1.5) +
        ggrepel::geom_text_repel(label.padding=0.1, max.iter=100000, min.segment.length = 0.25,
                                 force = 2, force_pull=2, size=2.25) +
        scale_y_continuous(limits = c(-4,22), breaks = c(0,5,10,15,20), 
                           labels = c('Birth', '5', '10','15', '20+'))+
        scale_color_viridis_c(option='magma', begin = 0, end = 0.9) + 
        facet_wrap(~Participant, ncol = 6) +
        xlab('')+
        ylab('Age Started Learning Language (Year)') +
        theme(panel.border = element_rect(fill=NA), 
              legend.position = 'bottom',
              strip.text = element_text(size = 8, face = 'bold'))

# ggsave('figs/age_started_learning.png', width = 7, height = 9.5, units = 'in', dpi=300)
```






---