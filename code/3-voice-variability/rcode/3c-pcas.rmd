---
title: 'Chapter 3: PCAs for varying passage lengths'
author: "Khia A. Johnson"
date: "7/23/2021"
output: html_document
---

# Imports & load data
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(ggthemes)
library(ggrepel)
library(parameters)
library(candisc)
library(data.table)
```

Set visualization defaults
```{r}
theme_set(theme_clean() + theme(
      legend.title = element_text(face = 'bold', size=10),
      legend.text = element_text(size=10),
      legend.position = 'bottom',
      axis.title.x = element_text(size = 10, face='bold'),
      axis.title.y = element_text(size = 10, face='bold'),
      strip.text = element_text(size=10, face='bold'),
      plot.background = element_blank()
    ))
```

Load the data
```{r}
df <- fread('../data/spice_voicesauce_processed.csv', sep = ',') 
grouping_vars <- read_csv('../data/spice_grouping_vars.csv')
```

Summarize range of samples across interviews
```{r}
df %>%
    group_by(File) %>%
    summarise(n=n()) %>%
    ungroup() %>%
    summarise(min(n), median(n), max(n))
```

```{r}
df_trunc <- df %>%
    group_by(File) %>%
    slice_head(n=20124)
nrow(df_trunc)
```

Function to run PCA for a single talker with promax rotation/standardization and process the output
```{r}
run_pca <- function(one_talker_df, filename, n=0) {

  # run the pca
  tX <- one_talker_df[,10:33]
  if (n>0) {
    n_components <- n
  } else {
    tX_pca_all <- principal_components(tX, n='all', rotation = "promax", standardize = TRUE)
    tX_var_all <- summary(tX_pca_all)
    tX_var_all <- rownames_to_column(setNames(data.frame(t(tX_var_all[,-1])), tX_var_all[,1]), 'Component')
    n_components <- nrow(filter(tX_var_all, Eigenvalues > mean(tX_var_all$Eigenvalues)*0.7))
  }
  tX_pca <- principal_components(tX, n = n_components, rotation = "promax", standardize = TRUE) 

  # add summary information and wrangle
  tX_var <-summary(tX_pca)
  tX_var <- rownames_to_column(setNames(data.frame(t(tX_var[,-1])), tX_var[,1]), 'Component')
  tX_pca <- as_tibble(tX_pca) %>% 
    pivot_longer(cols = starts_with('RC'), names_to = 'Component', values_to = "Loading") %>%
    left_join(tX_var, by = 'Component') %>%
    drop_na() %>%
    mutate(File=filename) %>%
    select(File, Variable, Component, Loading, Eigenvalues, Variance, Variance_Proportion, Complexity, Uniqueness, MSA)
  
  # clean up
  if (n>0) {
    rm(tX, tX_var)    
  } else {
    rm(tX, tX_pca_all, tX_var_all, tX_var)
  }

  return(tX_pca)
}
```


Using the function `run_pca()` function defined above, does PCA separately for each Talker/Language combo, and saves to `list_results`. *Note: prints file names as they finish running.*
```{r fig.height=4, fig.width=4}
list_results <- list()
files <- unique(df_trunc$File)

for (f in files) {
  this_file <- df_trunc %>% filter(File==f)
  this_pca <- run_pca(this_file, f)
  list_results[[f]] <- this_pca
  print(f)
}
```

```{r}
saveRDS(list_results, file='../data/pca_results_truncated.rds')
list_results <- readRDS(file='../data/pca_results_truncated.rds')
```


Wrangle the `list_results` object into a more useable dataframe, and save it.  
```{r message=FALSE, warning=FALSE}
results <- bind_rows(list_results)

results <- results %>%
  separate(File, into = c('Talker', 'Language', 'Order'), remove=FALSE) %>%
  left_join(grouping_vars, by='Variable') %>%
  group_by(File, Component) %>%
  arrange(desc(abs(Loading))) %>%
  mutate(number = 1, Bar_Order = cumsum(number)) %>%
  ungroup

variance_orders <- results %>%
  group_by(File, Component, Variance) %>%
  summarize() %>%
  group_by(File) %>%
  arrange(desc(Variance)) %>%
  mutate(number=1, Var_Order=cumsum(number)) %>%
  arrange(File) %>%
  select(File, Component, Var_Order)%>%
  ungroup

results <-left_join(results, variance_orders, by=c('File', 'Component'))
write_csv(results, '../data/pca_results_truncated.csv')
```

Clean up intermediate and auxiliary things
```{r}
rm(this_file, this_pca, f,variance_orders, grouping_vars)
```




**cleaned up code until here**

## Analyze PCA results
```{r}
results <- read_csv('../data/pca_results_truncated.csv')
```

```{r}
results
```

```{r}

```

Function to plot loadings
```{r, fig.width=4, fig.height=6}
return_loadings_plot <- function(subset) {
  
  subset <- subset %>%
    mutate(Loading= abs(Loading))
  
  lplot <- ggplot(subset, aes(y=Loading, x=Bar_Order, fill=Category, label=Variable)) +
      geom_col() +
      scale_fill_viridis_d(option='magma', begin = 0, end=0.75) +
      geom_text(angle=90, nudge_y = -0.5*(subset$Loading), colour = "white", fontface = "bold", size=3)+
      geom_label(aes(label=paste('Var:', round(Variance,3))), x= 0.6*max(subset$Bar_Order), y=-0.1, size=3, fill='white') +
      scale_y_continuous(limits = c(-0.25,1.1)) +
      facet_grid(Language~Var_Order) +
      theme_clean() +
      ylab('Loading') +
      xlab('') +
      theme(strip.text = element_text(face='bold', size=6),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank(),
            axis.title.x = element_blank(),
            legend.position = 'bottom',
            legend.title = element_blank(),
            legend.text = element_text(size=6),
            legend.key.size = unit(0.15, "in"),
            panel.border = element_rect(color = "lightgray", fill = NA, size = 0.5),
            plot.title = element_text(hjust = 0.5)) +
      guides(fill = guide_legend(nrow = 2))
  
  return(lplot)
} 
```

```{r fig.width=12, fig.height=6}
return_loadings_plot(filter(results, Talker=='VF19A', abs(Loading) > 0.45))
```

```{r}
results %>%
  filter(abs(Loading)>0.45) %>%
  select(Talker, Language, Variable, Variance, Var_Order) %>%
  mutate(Variance = round(Variance,2)) %>%
  pivot_wider(names_from = Variable, values_from = Variable) %>%
  group_by(Talker, Language) %>%
  summarize(n=n(), sv = sum(Variance)) %>%
  ungroup()  %>%
  pivot_wider(names_from = Language, values_from = n:sv) %>%
  write_csv('components_variances.csv')
```

```{r}
results %>%
  filter(abs(Loading)>0.45) %>%
  select(Talker, Language, Variable, Variance, Var_Order) %>%
  pivot_wider(names_from = Variable, values_from = Variable) %>%
  group_by(Talker, Language) %>%
  summarize(n=n()) %>%
  pivot_wider(names_from = Language, values_from = n) %>%
  mutate(d = as_factor(abs(Cantonese-English)) )%>%
  group_by(d) %>%
  summarise(n())
```

This is a bit messy, but its a useful structure for exploring the components.. a little better than it was
```{r}
results %>%
  filter(abs(Loading)>0.45) %>%
  select(Talker, Language, Variable, Variance, Var_Order) %>%
  pivot_wider(names_from = Variable, values_from = Variable) %>%
  select(Talker, Language, Variance, F0, F0_sd, F1, F1_sd, F2, F2_sd, F3, F3_sd, 
         F4, F4_sd, H1H2c, H1H2c_sd, H2H4c, H2H4c_sd, H2KH5Kc, H2KH5Kc_sd, 
         H42Kc, H42Kc_sd, CPP, CPP_sd, Energy, Energy_sd, SHR, SHR_sd
         ) %>%
  group_by(Talker, F0, F0_sd, F1, F1_sd, F2, F2_sd, F3, F3_sd, F4, F4_sd, H1H2c, 
           H1H2c_sd, H2H4c, H2H4c_sd, H2KH5Kc, H2KH5Kc_sd, H42Kc, H42Kc_sd, CPP, 
           CPP_sd, Energy, Energy_sd, SHR, SHR_sd
           ) %>%
  unite('Component', F0:SHR_sd, na.rm=TRUE, sep = " | ") %>%
  group_by(Talker, Component) %>% 
  summarise(n = n()) %>%
  group_by(Talker) %>%
  mutate(shared = n==2) %>%
  summarise(shared_count = sum(shared), shared_prop=2*sum(shared)/sum(n)) %>%
  write_csv('shared_comps.csv')
    # ungroup() %>%
  # summarise(min(shared_count), mean(shared_count), median(shared_count), max(shared_count),
  #           min(shared_prop), mean(shared_prop), median(shared_prop), max(shared_prop))

```

0.333--0.9167, M=0.667

```{r}
results %>%
  filter(abs(Loading)>0.45) %>%
  select(Talker, Language, Variable, Variance, Var_Order) %>%
  pivot_wider(names_from = Variable, values_from = Variable) %>%
  select(Talker, Language, Variance, F0, F0_sd, F1, F1_sd, F2, F2_sd, F3, F3_sd, 
         F4, F4_sd, H1H2c, H1H2c_sd, H2H4c, H2H4c_sd, H2KH5Kc, H2KH5Kc_sd, 
         H42Kc, H42Kc_sd, CPP, CPP_sd, Energy, Energy_sd, SHR, SHR_sd
         ) %>%
  group_by(Talker, F0, F0_sd, F1, F1_sd, F2, F2_sd, F3, F3_sd, F4, F4_sd, H1H2c, 
           H1H2c_sd, H2H4c, H2H4c_sd, H2KH5Kc, H2KH5Kc_sd, H42Kc, H42Kc_sd, CPP, 
           CPP_sd, Energy, Energy_sd, SHR, SHR_sd
           ) %>%
  unite('Component', F0:SHR_sd, na.rm=TRUE, sep = " | ") %>%
  group_by(Talker, Component) %>% 
  summarise(n = n()) %>%
  group_by(Talker) %>%
  mutate(shared = n==2) %>%
  summarise(shared_count = sum(shared), shared_prop=2*sum(shared)/sum(n))

```

```{r fig.width= 12, fig.height=6.5}
results %>%
  filter(abs(Loading)>0.45) %>%
  select(Talker, Language, Variable, Variance, Var_Order) %>%
  mutate(Variance = round(Variance, 3)) %>%
  pivot_wider(names_from = Variable, values_from = Variable) %>%
  select(Talker, Language, Variance, F0, F0_sd, F1, F1_sd, F2, F2_sd, F3, F3_sd, 
         F4, F4_sd, H1H2c, H1H2c_sd, H2H4c, H2H4c_sd, H2KH5Kc, H2KH5Kc_sd, 
         H42Kc, H42Kc_sd, CPP, CPP_sd, Energy, Energy_sd, SHR, SHR_sd
         ) %>%
  group_by(Talker, F0, F0_sd, F1, F1_sd, F2, F2_sd, F3, F3_sd, F4, F4_sd, H1H2c, 
           H1H2c_sd, H2H4c, H2H4c_sd, H2KH5Kc, H2KH5Kc_sd, H42Kc, H42Kc_sd, CPP, 
           CPP_sd, Energy, Energy_sd, SHR, SHR_sd
           ) %>%
  unite('Component', F0:SHR_sd, na.rm=TRUE, sep = " | ") %>%
  mutate(Component = str_replace_all(Component, '_sd', ' s.d.')) %>%
  group_by(Language, Component) %>%
  summarise(n=n(), minV = min(Variance), maxV = max(Variance)) %>%
  arrange(desc(maxV), Component) %>%
  ungroup() %>%
  write_csv('component_summary.csv')
 #  filter(n>10) %>%
 # ggplot(aes(y=Component, xmin=minV, xmax=maxV, color=Language)) +
 #  
 #  geom_linerange() +
 #  scale_color_viridis_d(option = 'magma', begin=0.9, end=0) +
 #    theme_clean()+
 #    theme(legend.position = 'none',
 #          panel.border = element_rect(color='black',fill=NA),
 #          strip.text = element_text(size=14, face='bold'))
```

```{r}
library(ggrepel)
```


```{r}
 ggplot(aes(x=mv, y=mpob, label=Component, color=n, size=n)) +
    geom_text_repel(segment.colour = NA, box.padding = 0.2) +
    scale_color_viridis_c(option = 'magma', begin=0.9, end=0) +
    facet_wrap(~Language) +
    ylab('Consistency (Mean Proportion short PCAs with component)')+
    xlab('Importance (Mean variance)')+
    theme_clean()+
    theme(legend.position = 'none',
          panel.border = element_rect(color='black',fill=NA),
          strip.text = element_text(size=14, face='bold'))
```



### Variable passage length stuff

Function to get the PCA loadings for a subset of the big dataframe
```{r}
get_pca_loadings_tibble_pl <- function(f) {
  m <- pl_results %>%
    filter(File==f) %>%
    select(Variable, Var_Order, Loading) %>%
    arrange(Var_Order, Variable) %>%
    pivot_wider(names_from = Var_Order, values_from = Loading)
  return(m[,2:ncol(m)]) 
}
```


```{r}
pl_list_results <- list()
lengths <- seq(from=500, to=nrow(test1), by=2000)
for (l in lengths) {
    print(l)
    this_length <- slice_head(test1, n=l)
    this_pca <- run_pca(this_length, paste('vf19a',l,sep='_'))
    pl_list_results[[l]] <- this_pca
  }

pl_results <- bind_rows(pl_list_results)

pl_variance_orders <- pl_results %>%
  group_by(File, Component, Variance) %>%
  summarize() %>%
  group_by(File) %>%
  arrange(desc(Variance)) %>%
  mutate(number=1, Var_Order=cumsum(number)) %>%
  arrange(File) %>%
  select(File, Component, Var_Order)%>%
  ungroup()

pl_results <- left_join(pl_results, pl_variance_orders, by=c('File', 'Component'))

all_pairs <- tibble(
    V1 = unique(pl_results$File),
    V2 = 'vf19a_60500'
)

# calc redundancy
xs <- list()
ys <- list()
px <- list()
py <- list()

for (i in 1:nrow(all_pairs)) {

  x_file <- all_pairs[[i, 1]]
  y_file <- all_pairs[[i, 2]]
  this_CCA <- cancor(
    x=get_pca_loadings_tibble_pl(x_file),
    y=get_pca_loadings_tibble_pl(y_file))
  this_red <- redundancy(this_CCA)

  px[[i]] <- x_file
  py[[i]] <- y_file
  xs[[i]] <- this_red$X.redun
  ys[[i]] <- this_red$Y.redun
  }

all_redundancy <- as_tibble(cbind(px,py,xs,ys)) %>% 
  unnest(c(px,py,xs,ys))

rm(px,py,xs,ys, x_file, y_file, this_CCA, this_red)
```


```{r}
all_redundancy %>%
    mutate(size=lengths) %>%
    pivot_longer(cols = xs:ys) %>%
    ggplot(aes(x=size, y=value, color=name)) +
    geom_point() +
    geom_line() +
    scale_color_colorblind()+
    theme_clean() +
    geom_vline(xintercept = 22433) +
    geom_vline(xintercept = 5150)

# %>%
#     ggplot(aes(x=xs,y=ys,label=px)) +
#     geom_text() +
#     geom_line()+
#     theme_clean()
```


---
